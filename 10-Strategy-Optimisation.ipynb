{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "# train_test_split.py\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import sklearn\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.qda import QDA\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from create_lagged_series import create_lagged_series\n",
    "\n",
    "# The test data is split into two parts: Before and after 1st Jan 2005.\n",
    "start_test = datetime.datetime(2005,1,1)\n",
    "\n",
    "# Create training and test sets\n",
    "X_train = X[X.index < start_test]\n",
    "X_test = X[X.index >= start_test]\n",
    "y_train = y[y.index < start_test]\n",
    "y_test = y[y.index >= start_test]\n",
    "\n",
    "\n",
    "# train_test_split.py\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a lagged series of the S&P500 US stock market index\n",
    "    snpret = create_lagged_series(\n",
    "    \"^GSPC\", datetime.datetime(2001,1,10),\n",
    "    datetime.datetime(2005,12,31), lags=5\n",
    "    )\n",
    "    # Use the prior two days of returns as predictor\n",
    "    # values, with direction as the response\n",
    "    X = snpret[[\"Lag1\",\"Lag2\"]]\n",
    "    y = snpret[\"Direction\"]\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.8, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create the (parametrised) models\n",
    "    print(\"Hit Rates/Confusion Matrices:\\n\")\n",
    "    models = [(\"LR\", LogisticRegression()),\n",
    "              (\"LDA\", LDA()),\n",
    "              (\"QDA\", QDA()),\n",
    "              (\"LSVC\", LinearSVC()),\n",
    "              (\"RSVM\", SVC(C = 1000000.0, \n",
    "                           cache_size = 200,\n",
    "                           class_weight = None,\n",
    "                           coef0 = 0.0, \n",
    "                           degree = 3, \n",
    "                           gamma = 0.0001,\n",
    "                           kernel = 'rbf',\n",
    "                           max_iter =- 1,\n",
    "                           probability = False,\n",
    "                           random_state = None,\n",
    "                           shrinking = True, \n",
    "                           tol = 0.001, \n",
    "                           verbose = False)),\n",
    "              (\"RF\", RandomForestClassifier(n_estimators = 1000, \n",
    "                                            criterion = 'gini',\n",
    "                                            max_depth = None,\n",
    "                                            min_samples_split = 2,\n",
    "                                            min_samples_leaf = 1,\n",
    "                                            max_features = 'auto',\n",
    "                                            bootstrap = True,\n",
    "                                            oob_score = False, \n",
    "                                            n_jobs = 1,\n",
    "                                            random_state = None,\n",
    "                                            verbose = 0))]\n",
    "    \n",
    "# Iterate through the models\n",
    "for m in models:\n",
    "\n",
    "    # Train each of the models on the training set\n",
    "    m[1].fit(X_train, y_train)\n",
    "\n",
    "    # Make an array of predictions on the test set\n",
    "    pred = m[1].predict(X_test)\n",
    "\n",
    "    # Output the hit-rate and the confusion matrix for each model\n",
    "    print(\"%s:\\n%0.3f\" % (m[0], m[1].score(X_test, y_test)))\n",
    "    print(\"%s\\n\" % confusion_matrix(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cross_validation' from 'sklearn.feature_selection' (c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\GIT-repository\\github-python-algo-trading-II\\python-algo-trading-II\\10-Strategy-Optimisation.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-python-algo-trading-II/python-algo-trading-II/10-Strategy-Optimisation.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-python-algo-trading-II/python-algo-trading-II/10-Strategy-Optimisation.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-python-algo-trading-II/python-algo-trading-II/10-Strategy-Optimisation.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m cross_validation\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-python-algo-trading-II/python-algo-trading-II/10-Strategy-Optimisation.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-python-algo-trading-II/python-algo-trading-II/10-Strategy-Optimisation.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msvm\u001b[39;00m \u001b[39mimport\u001b[39;00m SVC\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'cross_validation' from 'sklearn.feature_selection' (c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\__init__.py)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "# k_fold_cross_val.py\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_selection import cross_validation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from create_lagged_series import create_lagged_series\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create a lagged series of the S&P500 US stock market index\n",
    "    snpret = create_lagged_series(\"^GSPC\", \n",
    "                                  datetime.datetime(2001,1,10), \n",
    "                                  datetime.datetime(2005,12,31), \n",
    "                                  lags=5)\n",
    "    \n",
    "    # Use the prior two days of returns as predictor\n",
    "    # values, with direction as the response\n",
    "    X = snpret[[\"Lag1\",\"Lag2\"]]\n",
    "    y = snpret[\"Direction\"]\n",
    "    \n",
    "    # Create a k-fold cross validation object\n",
    "    kf = cross_validation.KFold(len(snpret),\n",
    "                                n_folds = 10,\n",
    "                                indices = False,\n",
    "                                shuffle = True,\n",
    "                                random_state = 42)\n",
    "    \n",
    "    # Use the kf object to create index arrays that\n",
    "    # state which elements have been retained for training\n",
    "    # and which elements have beenr retained for testing\n",
    "    # for each k-element iteration\n",
    "    for train_index, test_index in kf:\n",
    "        X_train = X.ix[X.index[train_index]]\n",
    "        X_test = X.ix[X.index[test_index]]\n",
    "        y_train = y.ix[y.index[train_index]]\n",
    "        y_test = y.ix[y.index[test_index]]\n",
    "        \n",
    "        # In this instance only use the\n",
    "        # Radial Support Vector Machine (SVM)\n",
    "        print(\"Hit Rate/Confusion Matrix:\")\n",
    "        model = SVC(C = 1000000.0, \n",
    "                    cache_size = 200,\n",
    "                    class_weight = None,\n",
    "                    coef0 = 0.0,\n",
    "                    degree = 3, \n",
    "                    gamma = 0.0001,\n",
    "                    kernel = 'rbf',\n",
    "                    max_iter =-1 ,\n",
    "                    probability = False,\n",
    "                    random_state = None,\n",
    "                    shrinking = True,\n",
    "                    tol = 0.001,\n",
    "                    verbose = False)\n",
    "        \n",
    "        # Train the model on the retained training data\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make an array of predictions on the test set\n",
    "        pred = model.predict(X_test)\n",
    "        \n",
    "        # Output the hit-rate and the confusion matrix for each model\n",
    "        print(\"%0.3f\" % model.score(X_test, y_test))\n",
    "        print(\"%s\\n\" % confusion_matrix(pred, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import ParameterGrid\n",
    "\n",
    "param_grid = {'C': [1, 10, 100, 1000], \n",
    "              'gamma': [0.001, 0.0001]}\n",
    "\n",
    "list(ParameterGrid(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "# grid_search.py\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import sklearn\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from create_lagged_series import create_lagged_series\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Create a lagged series of the S&P500 US stock market index\n",
    "    snpret = create_lagged_series(\"^GSPC\", \n",
    "                                  datetime.datetime(2001,1,10),\n",
    "                                  datetime.datetime(2005,12,31), lags=5)\n",
    "    \n",
    "    # Use the prior two days of returns as predictor\n",
    "    # values, with direction as the response\n",
    "    X = snpret[[\"Lag1\",\"Lag2\"]]\n",
    "    y = snpret[\"Direction\"]\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=0.5,\n",
    "                                                        random_state=42)\n",
    "    \n",
    "    # Set the parameters by cross-validation\n",
    "    tuned_parameters = [{'kernel': ['rbf'], \n",
    "                         'gamma': [1e-3, 1e-4],\n",
    "                         'C': [1, 10, 100, 1000]}]\n",
    "    \n",
    "    # Perform the grid search on the tuned parameters\n",
    "    model = GridSearchCV(SVC(C=1), tuned_parameters, cv=10)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Optimised parameters found on training set:\")\n",
    "    print(model.best_estimator_, \"\\n\")\n",
    "    print(\"Grid scores calculated on training set:\")\n",
    "    \n",
    "    for params, mean_score, scores in model.grid_scores_:\n",
    "        print(\"%0.3f for %r\" % (mean_score, params))\n",
    "        \n",
    "        \n",
    "# Optimised parameters found on training set:\n",
    "\n",
    "SVC(C = 1,\n",
    "    cache_size = 200, \n",
    "    class_weight = None,\n",
    "    coef0 = 0.0,\n",
    "    degree = 3,\n",
    "    gamma = 0.001,\n",
    "    kernel = 'rbf',\n",
    "    max_iter =- 1,\n",
    "    probability = False,\n",
    "    random_state = None,\n",
    "    shrinking = True,\n",
    "    tol = 0.001,\n",
    "    verbose = False)\n",
    "\n",
    "#Grid scores calculated on training set:\n",
    "\"\"\"\n",
    "0.541 for {'kernel': 'rbf', 'C': 1, 'gamma': 0.001}\n",
    "0.541 for {'kernel': 'rbf', 'C': 1, 'gamma': 0.0001}\n",
    "0.541 for {'kernel': 'rbf', 'C': 10, 'gamma': 0.001}\n",
    "0.541 for {'kernel': 'rbf', 'C': 10, 'gamma': 0.0001}\n",
    "0.541 for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001}\n",
    "0.541 for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001}\n",
    "0.538 for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}\n",
    "0.541 for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001}\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimising Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.intraday_mr import *\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_dir = 'stocks_data/' # CHANGE THIS!\n",
    "    symbol_list = ['AREX', 'WLL']\n",
    "    initial_capital = 100000.0\n",
    "    heartbeat = 0.0\n",
    "    start_date = datetime.datetime(2007, 11, 8, 10, 41, 0)\n",
    "\n",
    "    # Create the strategy parameter grid\n",
    "    # using the itertools cartesian product generator\n",
    "    strat_lookback = [50, 100, 200]\n",
    "    strat_z_entry = [2.0, 3.0, 4.0]\n",
    "    strat_z_exit = [0.5, 1.0, 1.5]\n",
    "    strat_params_list = list(product(strat_lookback, \n",
    "                                     strat_z_entry,\n",
    "                                     strat_z_exit))\n",
    "\n",
    "    # Create a list of dictionaries with the correct\n",
    "    # keyword/value pairs for the strategy parameters\n",
    "    strat_params_dict_list = [dict(ols_window=sp[0], \n",
    "                                   zscore_high=sp[1], \n",
    "                                   zscore_low=sp[2])\n",
    "    for sp in strat_params_list]\n",
    "    \n",
    "    # Carry out the set of backtests for all parameter combinations\n",
    "    backtest = Backtest(csv_dir,\n",
    "                        symbol_list,\n",
    "                        initial_capital,heartbeat,\n",
    "                        start_date,\n",
    "                        HistoricCSVDataHandlerHFT,\n",
    "                        SimulatedExecutionHandler,\n",
    "                        PortfolioHFT,\n",
    "                        IntradayOLSMRStrategy,  \n",
    "                        strat_params_list = strat_params_dict_list)\n",
    "        \n",
    "    backtest.simulate_trading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.backtest import *\n",
    "\n",
    "def _generate_trading_instances(self, strategy_params_dict):\n",
    "    \"\"\"\n",
    "    Generates the trading instance objects from\n",
    "    their class types.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating DataHandler, Strategy, Portfolio and ExecutionHandler for\")\n",
    "    print(\"strategy parameter list: %s...\" % strategy_params_dict)\n",
    "    \n",
    "    self.data_handler = self.data_handler_cls(self.events,\n",
    "                                              self.csv_dir,\n",
    "                                              self.symbol_list,\n",
    "                                              self.header_strings)\n",
    "    self.strategy = self.strategy_cls(self.data_handler, \n",
    "                                      self.events,\n",
    "                                      **strategy_params_dict)\n",
    "\n",
    "    self.portfolio = self.portfolio_cls(self.data_handler, \n",
    "                                        self.events,\n",
    "                                        self.start_date,\n",
    "                                        self.num_strats,\n",
    "                                        self.periods,\n",
    "                                        self.initial_capital)\n",
    "    \n",
    "    self.execution_handler = self.execution_handler_cls(self.events)\n",
    "    \n",
    "def simulate_trading(self):\n",
    "    \"\"\"\n",
    "    Simulates the backtest and outputs portfolio performance.\n",
    "    \"\"\"\n",
    "\n",
    "    out = open(\"output/opt.csv\", \"w\")\n",
    "    spl = len(self.strat_params_list)\n",
    "\n",
    "    for i, sp in enumerate(self.strat_params_list):\n",
    "        print(\"Strategy %s out of %s...\" % (i+1, spl))\n",
    "        self._generate_trading_instances(sp)\n",
    "        self._run_backtest()\n",
    "        stats = self._output_performance()\n",
    "        pprint.pprint(stats)\n",
    "        tot_ret = float(stats[0][1].replace(\"%\",\"\"))\n",
    "        cagr = float(stats[1][1].replace(\"%\",\"\"))\n",
    "        sharpe = float(stats[2][1])\n",
    "        max_dd = float(stats[3][1].replace(\"%\",\"\"))\n",
    "        dd_dur = int(stats[4][1])\n",
    "        out.write(\"%s,%s,%s,%s,%s,%s,%s,%s\\n\" % (sp[\"ols_window\"],\n",
    "                                                 sp[\"zscore_high\"],\n",
    "                                                 sp[\"zscore_low\"],\n",
    "                                                 tot_ret, \n",
    "                                                 cagr, \n",
    "                                                 sharpe,\n",
    "                                                 max_dd,\n",
    "                                                 dd_dur))\n",
    "    out.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "# plot_sharpe.py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_data_matrix(csv_ref, col_index):\n",
    "    data = np.zeros((3, 3))\n",
    "    for i in range(0, 3):\n",
    "        for j in range(0, 3):\n",
    "            data[i][j] = float(csv_ref[i*3+j][col_index])\n",
    "    \n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Open the CSV file and obtain only the lines\n",
    "    # with a lookback value of 100\n",
    "    csv_file = open(\"stocks_data/opt.csv\", \"r\").readlines()\n",
    "    csv_ref = [c.strip().split(\",\") for c in csv_file if c[:3] == \"100\"]\n",
    "    \n",
    "    data = create_data_matrix(csv_ref, 5)\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(data, cmap=plt.cm.Blues)\n",
    "    row_labels = [0.5, 1.0, 1.5]\n",
    "    column_labels = [2.0, 3.0, 4.0]\n",
    "    \n",
    "    for y in range(data.shape[0]):\n",
    "        for x in range(data.shape[1]):\n",
    "            plt.text(x + 0.5, y + 0.5, '%.2f' % data[y, x],\n",
    "                     horizontalalignment='center',\n",
    "                     verticalalignment='center',)\n",
    "    \n",
    "    plt.colorbar(heatmap)\n",
    "    \n",
    "    ax.set_xticks(np.arange(data.shape[0])+0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(data.shape[1])+0.5, minor=False)\n",
    "    \n",
    "    ax.set_xticklabels(row_labels, minor=False)\n",
    "    ax.set_yticklabels(column_labels, minor=False)\n",
    "    \n",
    "    plt.suptitle('Sharpe Ratio Heatmap', fontsize=18)\n",
    "    plt.xlabel('Z-Score Exit Threshold', fontsize=14)\n",
    "    plt.ylabel('Z-Score Entry Threshold', fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "# plot_drawdown.py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_data_matrix(csv_ref, col_index):\n",
    "    data = np.zeros((3, 3))\n",
    "    for i in range(0, 3):\n",
    "        for j in range(0, 3):\n",
    "            data[i][j] = float(csv_ref[i*3+j][col_index])\n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Open the CSV file and obtain only the lines\n",
    "    # with a lookback value of 100\n",
    "    csv_file = open(\"stocks_data/opt.csv\", \"r\").readlines()\n",
    "    csv_ref = [c.strip().split(\",\")for c in csv_file if c[:3] == \"100\"]\n",
    "    data = create_data_matrix(csv_ref, 6)\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(data, cmap = plt.cm.Reds)\n",
    "    row_labels = [0.5, 1.0, 1.5]\n",
    "    column_labels = [2.0, 3.0, 4.0]\n",
    "    \n",
    "    for y in range(data.shape[0]):\n",
    "        for x in range(data.shape[1]):\n",
    "            plt.text(x + 0.5, y + 0.5, '%.2f%%' % data[y, x],\n",
    "                     horizontalalignment='cente',\n",
    "                     verticalalignment='center',)\n",
    "    \n",
    "    plt.colorbar(heatmap)\n",
    "    \n",
    "    ax.set_xticks(np.arange(data.shape[0])+0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(data.shape[1])+0.5, minor=False)\n",
    "    ax.set_xticklabels(row_labels, minor=False)\n",
    "    ax.set_yticklabels(column_labels, minor=False)\n",
    "    \n",
    "    plt.suptitle('Maximum Drawdown Heatmap', fontsize=18)\n",
    "    plt.xlabel('Z-Score Exit Threshold', fontsize=14)\n",
    "    plt.ylabel('Z-Score Entry Threshold', fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74af3d2f25f1e46ebd2903d59225d79e4675ec224d56c01fc30cd168c2010d53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
